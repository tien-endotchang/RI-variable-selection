}
}
write.csv(tab, paste0(foldername, "res.", datasets[i], ".csv"), sep=",",
row.names = TRUE, col.names = TRUE)
}
# count = 1
# res.res = matrix(NA,100,4)
# while(count <= 100){
#   cat(paste0(count, "\n"))
#   training.ind = c(sample(which(y == 1), n.tv1), sample(which(y == 0), n.tv0))
#   xtv = x[training.ind, ]
#   ytv = y[training.ind]
#   xtest = x[-training.ind,]
#   ytest = y[-training.ind]
#
#   res.glmnet = cv.glmnet(xtv, ytv, alpha=1)
#   betahat = as.matrix(coef(res.glmnet, s = res.glmnet$lambda.min))
#
#   muhat.test = as.matrix(predict(res.glmnet, xtest, s = res.glmnet$lambda.min))
#   nzs = colSums(betahat!=0)
#   yhat = ifelse(muhat.test > 0.5, 1, 0)
#   acc = colMeans(yhat == ytest)
#   tpos = colSums(ytest * yhat)
#   fpos = colSums(yhat) - tpos
#   tneg = colSums((1 - ytest) * (1 - yhat))
#   fneg = colSums(1 - yhat) - tneg
#   sens = tpos / (tpos + fneg)
#   spec = tneg / (fpos + tneg)
#   bacc = (sens + spec) / 2
#   F1 = 2 * tpos / (2 * tpos + fpos + fneg)
#   res.res[count,1:4] = c(acc,bacc,F1,nzs)
#   count = count + 1
# }
# round(apply(res.res, 2, mean), 2)
# round(apply(res.res, 2, sd)/10, 2)
# sim.obj$nzs[[1]][cbind(1:nrep, sim.obj$tun.val[, 1])]
# sim.obj$bacc[[1]][cbind(1:nrep, sim.obj$tun.val[, 1])]
# sim.obj$acc[[1]][cbind(1:nrep, sim.obj$tun.val[, 1])]ㄏ
paste0(foldername, "res.", datasets[i], ".csv")
tab = matrix(NA, nrow=length(reg.funs), ncol=3)
row.names(tab) = names(reg.funs)
metrics = c("err.test", "nzs", "runtime")
colnames(tab) = metrics
for(j in 1:length(metrics)){
if(j < length(metrics)){
z = sim.obj[[metrics[j]]]
res = tune.and.aggregate(sim.obj, z)
tab[, j] = paste0(round(res$z.val.ave, 4), " (", round(res$z.val.std, 4), ")")
}else{
z = sim.obj[[metrics[j]]]
res = matrix(unlist(z), ncol=length(reg.funs))
tab[, j] = paste0(round(colMeans(res), 2), " (", round(apply(res*1000, 2, sd) / sqrt(nrep), 2), ")")
}
}
write.csv(tab, paste0(foldername, "res.", datasets[i], ".csv"), sep=",",
row.names = TRUE, col.names = TRUE)
tab
x = as.matrix(x)
y = as.matrix(y)
n = nrow(x)
p = ncol(x)
n.tv = round(0.8 * n)
training.ind = sample(1:n, n.tv)
xtv = x[training.ind, ]
ytv = y[training.ind]
xtest = x[-training.ind,]
ytest = y[-training.ind]
n.t = round(0.8 * n.tv)
train.ind = sample(1:n.tv, n.t)
xtrain = xtv[train.ind, ]
ytrain =  ytv[train.ind]
xval = xtv[-train.ind,]
yval = ytv[-train.ind]
j=5
reg.obj = reg.funs[[j]](xtrain, ytrain)
reg.obj
reg.funs[[j]]
# Grab the estimated coefficients, and the predicted values on the
# training and validation sets
betahat = as.matrix(coef(reg.obj))
m = ncol(betahat); nc = nrow(betahat)
# Check for intercept
if (nc == p+1) {
intercept = TRUE
betahat0 = betahat[1,]
betahat = betahat[-1,]
}
nc == p+1
intercept = TRUE
betahat0 = betahat[1,]
betahat = betahat[-1,]
muhat.train = as.matrix(predict(reg.obj, xtrain))
muhat.val = as.matrix(predict(reg.obj, xval))
muhat.test = as.matrix(predict(reg.obj, xtest))
muhat.test
!filled[j]
colMeans((muhat.test - ytest)^2)
colSums(betahat!=0)
colMeans((muhat.test - ytest)^2)
muhat.test
ytest
## Real-world dataset Examples
# --------------------------------------------------------------------------
# SCRIPT SETUP: LOADING FUNCTIONS AND LIBRARIES
#
# This script should be run with the R working directory set to the root
# of the project folder (e.g., '~/RI-variable-selection/').
setwd("G:/其他電腦/我的筆記型電腦/PhD/Journal Paper/CRI-feature-selection/RI-variable-selection/RI-variable-selection")
# --------------------------------------------------------------------------
rm(list = ls())
# --- 1. Source R files from the external Hastie et al. code ---
# These are the original, unmodified benchmark functions.
message("Loading benchmark functions from external/hastie_code/R/...")
external_files <- list.files("external/hastie_code/R",
pattern = "\\.R$",
full.names = TRUE,
ignore.case = TRUE)
sapply(external_files, source)
# --- 2. Source our own custom and modified R functions ---
# These include our implementations of RI methods and modified simulation runners.
message("Loading custom functions from R/...")
custom_files <- list.files("R",
pattern = "\\.R$",
full.names = TRUE,
ignore.case = TRUE)
sapply(custom_files, source)
# --- 3. Load the compiled C/Fortran code (if necessary) ---
# This step is needed for the original benchmark code that relies on compiled functions.
# Note: This may require the user to have compilation tools installed.
dll_path <- file.path("src", "matrixcomps.dll")
if (file.exists(dll_path)) {
message(paste("Loading dynamic library:", dll_path))
dyn.load(dll_path)
} else {
warning(paste("Dynamic library not found at:", dll_path,
"\nSome benchmark functions may not work without compilation."))
}
glmnet::glmnet.control(fdev=0)
message("Setup complete. Starting simulations...")
#------------------------------------------------------------------------------#
# read file
# base_dir = "G:/其他電腦/我的筆記型電腦/PhD/Argon Lab/Revise Zi-Xin's Code/Real Case Study/"
# x = read.csv(paste0(base_dir, "AML/Data/AMLALLx.csv"), header = FALSE)
# y = read.csv(paste0(base_dir, "AML/Data/AMLALLy.csv"), header = FALSE)
# Install the library if you haven't
# if (!requireNamespace("BiocManager", quietly = TRUE))
#   install.packages("BiocManager")
# BiocManager::install("rat2302.db")
# Load library
# library(rat2302.db)
# setwd("G:/其他電腦/我的筆記型電腦/PhD/meeting/20251208-/")
#
# dat = read.csv("eyedata.csv", header=FALSE, row.names=1)[1:31042, ]
# probes = row.names(dat)
# # gene_symbols <- select(rat2302.db, keys=probes, columns="SYMBOL", keytype="PROBEID")
# # print(gene_symbols)
# # gene_symbols[gene_symbols$PROBEID=="1389163_at",]
# y_id = which(probes=="1389163_at")
# dat.t = t(as.matrix(dat))
#
# x = as.matrix(dat.t[, -y_id])
# y = as.matrix(dat.t[, y_id])
#
# write.table(x, "rateyex.csv", sep=",", row.names=F, col.names=F)
# write.table(y, "rateyey.csv", sep=",", row.names=F, col.names=F)
# n = nrow(x)
# n_tv = round(0.8 * n)
# set.seed(42)
#
# # train_val_ind = c(sample(which(y == 1), n_tv1), sample(which(y == 0), n_tv0))
# train_val_ind = sample(1:n, n_tv)
# xtv = x[train_val_ind, ]
# ytv = y[train_val_ind]
# xtest = x[-train_val_ind,]
# ytest = y[-train_val_ind]
#
# # training
#
# n_t = round(0.8 * n_tv)
#
# train_ind = sample(1:n_tv, n_t)
# xtrain = xtv[train_ind, ]
# ytrain =  ytv[train_ind]
# # ytrain =  scale(ytv[train_ind], center=TRUE, scale=FALSE)
# # mytrain = attr(ytrain, "scaled:center"); ytrain = as.numeric(ytrain)
# xval = xtv[-train_ind,]
# yval = ytv[-train_ind]
#
# p = ncol(xtrain)
# reg.obj = lscri(xtrain, ytrain, intercept=T)
# reg.obj = lscriz(xtrain, ytrain, intercept=T)
# reg.obj = lsSIS(xtrain, ytrain, intercept=T)
#
# reg.obj = lasso(xtrain, ytrain, intercept=FALSE, nlam=100)
# # reg.obj = ridgecri(x, y, intercept = T, nlam = 20)
# betahat = as.matrix(coef(reg.obj))
# m = ncol(betahat); nc = nrow(betahat)
#
# muhat.train = as.matrix(predict(reg.obj,xtrain))
# muhat.val = as.matrix(predict(reg.obj,xval))
# muhat.test = as.matrix(predict(reg.obj,xtest))
#
#
#
# best = which.min(colMeans((muhat.val -yval)^2))
# colMeans((muhat.test - ytest)^2)[best]
# colSums(betahat!=0)
#
# acc = mean(yhat_val == yval)
# tpos = sum(yval * (yhat_val))
# fpos = sum(yhat_val) - tpos
# tneg = sum((1 - yval) * (1 - yhat_val))
# fneg = sum(1 - yhat_val) - tneg
# sens = tpos / (tpos + fneg)
# spec = tneg / (fpos + tneg)
# auc = (sens + spec) / 2
# F1 = 2 * tpos / (2 * tpos + fpos + fneg)
#------------------------------------------------------------------------------#
run.realdata.reg = function(x, y, reg.funs, nrep=50, seed=NULL, verbose=FALSE,
file=NULL, file.rep=5) {
this.call = match.call()
if (!is.null(seed)) set.seed(seed)
N = length(reg.funs)
reg.names = names(reg.funs)
if (is.null(reg.names)) reg.names = paste("Method",1:N)
err.train = err.val = err.test = nzs = runtime = vector(mode="list",length=N)
names(err.train) = names(err.val) = names(err.test) =
names(nzs) = names(runtime) = reg.names
for (j in 1:N) {
err.train[[j]] = err.val[[j]] = err.test[[j]] = nzs[[j]] = runtime[[j]] =
matrix(NA,nrep,1)
}
filled = rep(FALSE,N)
x = as.matrix(x)
y = as.matrix(y)
n = nrow(x)
p = ncol(x)
n.tv = round(0.8 * n)
# Loop through the repetitions
for (i in 1:nrep) {
if (verbose) {
cat(sprintf("Real-world example %i (of %i) ...\n", i, nrep))
cat("  Splitting data ...\n")
}
training.ind = sample(1:n, n.tv)
xtv = x[training.ind, ]
ytv = y[training.ind]
xtest = x[-training.ind,]
ytest = y[-training.ind]
n.t = round(0.8 * n.tv)
train.ind = sample(1:n.tv, n.t)
xtrain = xtv[train.ind, ]
ytrain =  ytv[train.ind]
xval = xtv[-train.ind,]
yval = ytv[-train.ind]
# Loop through the regression methods
for (j in 1:N) {
if (verbose) {
cat(sprintf("  Applying regression method %i (of %i) ...\n",
j,N))
}
tryCatch({
# Apply the regression method in hand
runtime[[j]][i] = system.time({
reg.obj = reg.funs[[j]](xtrain, ytrain)
})[1]
# Grab the estimated coefficients, and the predicted values on the
# training and validation sets
betahat = as.matrix(coef(reg.obj))
m = ncol(betahat); nc = nrow(betahat)
# Check for intercept
if (nc == p+1) {
intercept = TRUE
betahat0 = betahat[1,]
betahat = betahat[-1,]
}
else intercept = FALSE
muhat.train = as.matrix(predict(reg.obj, xtrain))
muhat.val = as.matrix(predict(reg.obj, xval))
muhat.test = as.matrix(predict(reg.obj, xtest))
# Populate empty matrices for our metrics, of appropriate dimension
if (!filled[j]) {
err.train[[j]] = err.val[[j]] = err.test[[j]] = nzs[[j]] = matrix(NA,nrep,m)
filled[j] = TRUE
# N.B. Filling with NAs is important, because the filled flag could
# be false for two reasons: i) we are at the first iteration, or ii)
# we've failed in all previous iters to run the regression method
}
# Record all of our metrics
err.train[[j]][i,] = colMeans((muhat.train - ytrain)^2)
err.val[[j]][i,] = colMeans((muhat.val - yval)^2)
err.test[[j]][i,] = colMeans((muhat.test - ytest)^2)
nzs[[j]][i,] = colSums(betahat!=0)
}, error = function(err) {
if (verbose) {
cat(paste("    Oops! Something went wrong, see error message",
"below; recording all metrics here as NAs ...\n"))
cat("    ***** Error message *****\n")
cat(sprintf("    %s\n",err$message))
cat("    *** End error message ***\n")
}
# N.B. No need to do anything, the metrics are already filled with NAs
})
}
# Save intermediate results?
if (!is.null(file) && file.rep > 0 && i %% file.rep == 0) {
saveRDS(enlist(err.train,err.val,err.test,nzs,runtime),file=file)
}
}
# Save results now (in case of an error that might occur below)
out = enlist(err.train,err.val,err.test,nzs,runtime)
if (!is.null(file)) saveRDS(out, file)
# Tune according to validation error, and according to test error
out = choose.tuning.params(out)
# Save final results
out = c(out,list(call=this.call))
class(out) = "real"
if (!is.null(file)) { saveRDS(out, file); invisible(out) }
else return(out)
}
base.dir = "G:/其他電腦/我的筆記型電腦/PhD/meeting/20251208-/gene expression dataset/"
datasets = c("rateye")
foldername = paste0(base.dir, "results/")
dir.create(file.path(foldername), showWarnings = TRUE, recursive=TRUE)
nrep = 100
seed = 42
for(i in 1:length(datasets)){
cat(sprintf("Real-world dataset example: %s (%i of %i) ...\n", datasets[i], i, length(datasets)))
x = read.csv (paste0(base.dir, datasets[i], "/", datasets[i], "x.csv"), header = FALSE)
y = read.csv (paste0(base.dir, datasets[i], "/", datasets[i], "y.csv"), header = FALSE)
reg.funs = list()
reg.funs[["Lasso"]] = function(x,y) lasso(x, y, intercept = T, nlam = 2*nrow(x))
# reg.funs[["Forward stepwise"]] = function(x,y) fs.mod(x, y, intercept = T)
reg.funs[["Relaxed lasso"]] = function(x,y) lasso(x, y, intercept = T, nrelax = 10, nlam = 2*nrow(x))
# reg.funs[["LS-CRI"]] = function(x,y) lscri(x, y, intercept = T)
reg.funs[["LS-CRI.Z"]] = function(x,y) lscriz(x, y, intercept = T)
reg.funs[["LS-CAR"]] = function(x,y) lscar(x, y, intercept = T)
reg.funs[["LS-SIS"]] = function(x,y) lsSIS(x, y, intercept = T)
# reg.funs[["Ridge-CRI"]] = function(x,y) ridgecri(x, y, intercept = T, nlam = 20)
reg.funs[["Ridge-CRI.Z"]] = function(x,y) ridgecriz(x, y, intercept = T, nlam = 20)
reg.funs[["Ridge-CAR"]] = function(x,y) ridgecar(x, y, intercept = T, nlam = 20)
reg.funs[["Ridge-SIS"]] = function(x,y) ridgeSIS(x, y, intercept = T, nlam = 20)
file = paste0(foldername, "res.", datasets[i], ".rds")
sim.obj = run.realdata.reg(x, y, reg.funs=reg.funs, nrep=nrep, seed=seed, verbose=TRUE, file=file)
tab = matrix(NA, nrow=length(reg.funs), ncol=3)
row.names(tab) = names(reg.funs)
metrics = c("err.test", "nzs", "runtime")
colnames(tab) = metrics
for(j in 1:length(metrics)){
if(j < length(metrics)){
z = sim.obj[[metrics[j]]]
res = tune.and.aggregate(sim.obj, z)
tab[, j] = paste0(round(res$z.val.ave, 4), " (", round(res$z.val.std, 4), ")")
}else{
z = sim.obj[[metrics[j]]]
res = matrix(unlist(z), ncol=length(reg.funs))
tab[, j] = paste0(round(colMeans(res), 2), " (", round(apply(res*1000, 2, sd) / sqrt(nrep), 2), ")")
}
}
write.csv(tab, paste0(foldername, "res.", datasets[i], ".csv"), sep=",",
row.names = TRUE, col.names = TRUE)
}
# count = 1
# res.res = matrix(NA,100,4)
# while(count <= 100){
#   cat(paste0(count, "\n"))
#   training.ind = c(sample(which(y == 1), n.tv1), sample(which(y == 0), n.tv0))
#   xtv = x[training.ind, ]
#   ytv = y[training.ind]
#   xtest = x[-training.ind,]
#   ytest = y[-training.ind]
#
#   res.glmnet = cv.glmnet(xtv, ytv, alpha=1)
#   betahat = as.matrix(coef(res.glmnet, s = res.glmnet$lambda.min))
#
#   muhat.test = as.matrix(predict(res.glmnet, xtest, s = res.glmnet$lambda.min))
#   nzs = colSums(betahat!=0)
#   yhat = ifelse(muhat.test > 0.5, 1, 0)
#   acc = colMeans(yhat == ytest)
#   tpos = colSums(ytest * yhat)
#   fpos = colSums(yhat) - tpos
#   tneg = colSums((1 - ytest) * (1 - yhat))
#   fneg = colSums(1 - yhat) - tneg
#   sens = tpos / (tpos + fneg)
#   spec = tneg / (fpos + tneg)
#   bacc = (sens + spec) / 2
#   F1 = 2 * tpos / (2 * tpos + fpos + fneg)
#   res.res[count,1:4] = c(acc,bacc,F1,nzs)
#   count = count + 1
# }
# round(apply(res.res, 2, mean), 2)
# round(apply(res.res, 2, sd)/10, 2)
# sim.obj$nzs[[1]][cbind(1:nrep, sim.obj$tun.val[, 1])]
# sim.obj$bacc[[1]][cbind(1:nrep, sim.obj$tun.val[, 1])]
# sim.obj$acc[[1]][cbind(1:nrep, sim.obj$tun.val[, 1])]ㄏ
sim.obj$nzs[[1]][cbind(1:nrep, sim.obj$tun.val[, 1])]
sim.obj$err.test[[1]][cbind(1:nrep, sim.obj$tun.val[, 1])]
boxplot(sim.obj$err.test[[1]][cbind(1:nrep, sim.obj$tun.val[, 1])],
sim.obj$err.test[[3]][cbind(1:nrep, sim.obj$tun.val[, 3])])
boxplot(sim.obj$err.test[[1]][cbind(1:nrep, sim.obj$tun.val[, 1])],
sim.obj$err.test[[3]][cbind(1:nrep, sim.obj$tun.val[, 3])],
sim.obj$err.test[[5]][cbind(1:nrep, sim.obj$tun.val[, 5])]
)
x = as.matrix(x)
y = as.matrix(y)
n = nrow(x)
p = ncol(x)
n.tv = round(0.8 * n)
sample(1:n, n.tv)
training.ind = sample(1:n, n.tv)
xtv = x[training.ind, ]
ytv = y[training.ind]
xtest = x[-training.ind,]
ytest = y[-training.ind]
n.t = round(0.8 * n.tv)
train.ind = sample(1:n.tv, n.t)
xtrain = xtv[train.ind, ]
ytrain =  ytv[train.ind]
xval = xtv[-train.ind,]
yval = ytv[-train.ind]
xtrain
j
reg.obj = reg.funs[[j]](xtrain, ytrain)
# Grab the estimated coefficients, and the predicted values on the
# training and validation sets
betahat = as.matrix(coef(reg.obj))
m = ncol(betahat); nc = nrow(betahat)
# Check for intercept
if (nc == p+1) {
intercept = TRUE
betahat0 = betahat[1,]
betahat = betahat[-1,]
}
muhat.train = as.matrix(predict(reg.obj, xtrain))
muhat.val = as.matrix(predict(reg.obj, xval))
muhat.test = as.matrix(predict(reg.obj, xtest))
muhat.test
colMeans((muhat.test - ytest)^2)
colMeans((muhat.test - ytest)^2) / var(ytest)
head(ytest)
boxplot(ytest)
colMeans((muhat.train - ytrain)^2) / var(ytrain)
colMeans((muhat.test - ytest)^2) / var(ytest)
1-colMeans((muhat.test - ytest)^2) / var(ytest)
colMeans((muhat.test - ytest)^2)
var(y)
var(ytest)
var(yval)
1 - colMeans((muhat.train - ytrain)^2) / var(ytrain)
1 - colMeans((muhat.test - ytest)^2) / var(ytest)
1 - colMeans((muhat.val - yval)^2) / var(yval)
ytest
muhat.test
which.min(colMeans((muhat.val - yval)^2))
muhat.test[, 48]
plot(muhat.test[, 48], ytest)
colMeans((muhat.val - yval)^2)
plot(muhat.val[, 48], yval)
reg.funs
j=6
reg.obj = reg.funs[[j]](xtrain, ytrain)
# Grab the estimated coefficients, and the predicted values on the
# training and validation sets
betahat = as.matrix(coef(reg.obj))
m = ncol(betahat); nc = nrow(betahat)
# Check for intercept
if (nc == p+1) {
intercept = TRUE
betahat0 = betahat[1,]
betahat = betahat[-1,]
}
muhat.train = as.matrix(predict(reg.obj, xtrain))
muhat.val = as.matrix(predict(reg.obj, xval))
muhat.test = as.matrix(predict(reg.obj, xtest))
colMeans((muhat.val - yval)^2)
which.min(colMeans((muhat.val - yval)^2))
colMeans((muhat.test - ytest)^2)[1482]
(1-colMeans((muhat.test - ytest)^2)/var(ytest))[1482]
points(muhat.test[, 1482], ytest, col="red")
apply(x, 2, var)
which.max(apply(x, 2, var))
betahat
dim(betahat)
which(betahat!=0)
which(betahat[,1482]!=0)
res.rateye <- readRDS("G:/其他電腦/我的筆記型電腦/PhD/meeting/20251208-/gene expression dataset/results/res.rateye.rds")
res.rateye$nzs$Lasso
sim.obj=res.rateye
sim.obj$nzs[[1]][cbind(1:nrep, sim.obj$tun.val[, 1])]
mean(sim.obj$nzs[[1]][cbind(1:nrep, sim.obj$tun.val[, 1])]
)
boxplot(sim.obj$nzs[[1]][cbind(1:nrep, sim.obj$tun.val[, 1])]
)
boxplot(sim.obj$nzs[[1]][cbind(1:nrep, sim.obj$tun.val[, 1])],
sim.obj$nzs[[3]][cbind(1:nrep, sim.obj$tun.val[, 3])],
sim.obj$nzs[[5]][cbind(1:nrep, sim.obj$tun.val[, 5])]
)
library(flare)
??eyedata
eyedata
?eyedata
x[1:5, 1:5]
data(eyedata)
x
y
log(y)
var(y)
n.tv
sample(1:n.tv, n.t)
xtv
i
x = read.csv (paste0(base.dir, datasets[i], "/", datasets[i], "x.csv"), header = FALSE)
